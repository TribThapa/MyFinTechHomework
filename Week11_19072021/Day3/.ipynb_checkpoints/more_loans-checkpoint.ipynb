{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Loans\n",
    "\n",
    "In this activity you will practice using random and SMOTE oversampling in combination with logistic regression to predict whether or not someone is likely to default on their credit card loans in a given month given demographic information. \n",
    "\n",
    "ln_balance_limit is the log of the maximum balance they can have on the card; 1 is female, 0 male for sex; the education is denoted: 1 = graduate school; 2 = university; 3 = high school; 4 = others; 1 is married and 0 single for marriage; default_next_month is whether the person defaults in the following month (1 yes, 0 no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from path import Path\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path(r'C:\\Users\\TribThapa\\Desktop\\Thapa\\ResearchFellow\\Courses\\FinTech_Bootcamp_MonashUni2021\\monu-mel-virt-fin-pt-05-2021-u-c\\Activities\\Week 11\\3\\04-Stu_Do_More_Loans\\Resources\\cc_default.csv')\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [i for i in df.columns if i not in (\"ID\", \"default_next_month\")]\n",
    "X = df[x_cols]\n",
    "y = df[\"default_next_month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 17532, 1: 4968})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ln_balance_limit': 1, 'sex': 1, 'education': 1, 'marriage': 1, 'age': 1})\n",
      "Counter({0: 17532, 1: 17532})\n"
     ]
    }
   ],
   "source": [
    "# Fit the RandomOverSampler to the data and check the count of each class\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"{Counter(X_resampled)}\")\n",
    "\n",
    "print(f\"{Counter(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a logistic regression model using random oversampled data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "\n",
    "model = LogisticRegression(C=1.0,\n",
    "                           class_weight=None, \n",
    "                           dual=False, \n",
    "                           fit_intercept=True,\n",
    "                           intercept_scaling=1, \n",
    "                           l1_ratio=None,\n",
    "                           max_iter=100,\n",
    "                           n_jobs=None,\n",
    "                           penalty='l2',\n",
    "                           random_state=1,                           \n",
    "                           solver='lbfgs', \n",
    "                           tol=0.0001,\n",
    "                           verbose=0,\n",
    "                           warm_start=False)\n",
    "\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions 0</th>\n",
       "      <th>Predictions 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>3744</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>745</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predictions 0  Predictions 1\n",
       "Actual 0           3744           2088\n",
       "Actual 1            745            923"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predictions 0\", \"Predictions 1\"])\n",
    "\n",
    "cm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5976663113953282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the balanced accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_acc_score = balanced_accuracy_score(y_test, predictions)\n",
    "\n",
    "balanced_acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.64      0.73      5832\n",
      "           1       0.31      0.55      0.39      1668\n",
      "\n",
      "    accuracy                           0.62      7500\n",
      "   macro avg       0.57      0.60      0.56      7500\n",
      "weighted avg       0.72      0.62      0.65      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.83      0.64      0.55      0.73      0.60      0.36      5832\n",
      "          1       0.31      0.55      0.64      0.39      0.60      0.35      1668\n",
      "\n",
      "avg / total       0.72      0.62      0.57      0.65      0.60      0.36      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the imbalanced classification report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_resampled: Counter({'ln_balance_limit': 1, 'sex': 1, 'education': 1, 'marriage': 1, 'age': 1})\n",
      "y_resampled: Counter({0: 17532, 1: 17532})\n"
     ]
    }
   ],
   "source": [
    "# Fit the SMOTE model to the data and check the count of each class\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled2, y_resampled2 = SMOTE(random_state=1, sampling_strategy=1.0).fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"X_resampled: {Counter(X_resampled2)}\")\n",
    "print(f\"y_resampled: {Counter(y_resampled2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a logistic regression model using the SMOTE resampled data\n",
    "model = LogisticRegression(C=1.0,\n",
    "                           class_weight=None, \n",
    "                           dual=False,\n",
    "                           fit_intercept=True,\n",
    "                           intercept_scaling=1,\n",
    "                           l1_ratio=None, \n",
    "                           max_iter=100,\n",
    "                           n_jobs=None, \n",
    "                           penalty='l2',\n",
    "                           random_state=1,\n",
    "                           solver='lbfgs',\n",
    "                           tol=0.0001, \n",
    "                           verbose=0,\n",
    "                           warm_start=False)\n",
    "\n",
    "model.fit(X_resampled2, y_resampled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction 0</th>\n",
       "      <th>Prediction 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>3744</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>745</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Prediction 0  Prediction 1\n",
       "Actual 0          3744          2088\n",
       "Actual 1           745           923"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "predictions2 = model.predict(X_test)\n",
    "\n",
    "cm2 = confusion_matrix(y_test, predictions2)\n",
    "\n",
    "cm_df2 = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Prediction 0\", \"Prediction 1\"])\n",
    "\n",
    "cm_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975337014339146"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the balanced accuracy score\n",
    "balanced_acc_score2 = balanced_accuracy_score(y_test, predictions2)\n",
    "\n",
    "balanced_acc_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.83      0.63      0.56      0.72      0.60      0.36      5832\n",
      "          1       0.30      0.56      0.63      0.40      0.60      0.35      1668\n",
      "\n",
      "avg / total       0.72      0.62      0.58      0.65      0.60      0.36      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the imbalanced classification report\n",
    "print(classification_report_imbalanced(y_test, predictions2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
