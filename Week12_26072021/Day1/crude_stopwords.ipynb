{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Do: Crude Stopwords\n",
    "For this activity, create a function that takes in an article and outputs a list of words that is free of stopwords and any non-letter characters. After looking at the results, define your own list of stopwords to add to the NLTK default set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TribThapa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crude_article = reuters.raw(fileids=reuters.fileids(categories='crude')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(crude_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwording and regex \n",
    "# Complete a function called clean_text that takes in a document and returns a list of words that does not include stopwords or any non-letter characters. \n",
    "\n",
    "def clean_text(article):\n",
    "\n",
    "    regex = re.compile(\"[^a-zA-Z ]\") # removing spaces \n",
    "    \n",
    "    sw = set(stopwords.words('english')) # removing The, If, When, it\n",
    "    \n",
    "    re_clean = regex.sub('', article) # replace characters above with spaces\n",
    "    \n",
    "    words = word_tokenize(re_clean)\n",
    "\n",
    "    result = [word.lower() for word in words if word.lower() not in sw]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store result of running clean_text\n",
    "clean_test_result = clean_text(crude_article)\n",
    "\n",
    "#clean_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "\n",
      "{'prime', 'turkeys', 'nato', 'confined', 'exploration', 'agreement', 'last', 'territorrial', 'could', 'averted', 'nazmi', 'disclosed', 'rights', 'greek', 'international', 'search', 'crisis', 'issue', 'due', 'ministry', 'bilateral', 'sea', 'aegean', 'countries', 'effect', 'solve', 'court', 'including', 'dialogue', 'today', 'legal', 'negotiations', 'shelf', 'dispute', 'stemmed', 'month', 'foreign', 'athens', 'reply', 'turgut', 'basicly', 'disputes', 'andreas', 'turkish', 'announced', 'calls', 'oil', 'faceoff', 'greece', 'ambassador', 'work', 'would', 'ozal', 'statement', 'historic', 'papandreou', 'sent', 'members', 'solved', 'two', 'contents', 'said', 'turkey', 'armed', 'repeatedly', 'created', 'waters', 'research', 'justice', 'confrontation', 'political', 'security', 'week', 'continental', 'economy', 'solution', 'planned', 'crises', 'found', 'akiman', 'minister', 'also', 'meet', 'message', 'approached', 'opportunity', 'latest'}\n"
     ]
    }
   ],
   "source": [
    "# Print unique words (a set of the result)\n",
    "print(len(clean_test_result))\n",
    "print()\n",
    "print(set(clean_test_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second iteration, with custom stopwords\n",
    "def clean_text(article):\n",
    "    regex2 = re.compile(\"turk \")\n",
    "    \n",
    "    sw = set(stopwords.words('english'))\n",
    "    \n",
    "    clean = regex2.sub('', article)\n",
    "    \n",
    "    words2 = word_tokenize(clean)\n",
    "    \n",
    "    result = [word.lower() for word in words2 if word.lower() not in sw]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " \"'s\",\n",
       " ',',\n",
       " '.',\n",
       " '``',\n",
       " 'aegean',\n",
       " 'agreement',\n",
       " 'akiman',\n",
       " 'also',\n",
       " 'ambassador',\n",
       " 'andreas',\n",
       " 'announced',\n",
       " 'approached',\n",
       " 'armed',\n",
       " 'athens',\n",
       " 'averted',\n",
       " 'basicly',\n",
       " 'bilateral',\n",
       " 'calls',\n",
       " 'confined',\n",
       " 'confrontation',\n",
       " 'contents',\n",
       " 'continental',\n",
       " 'could',\n",
       " 'countries',\n",
       " 'court',\n",
       " 'created',\n",
       " 'crises',\n",
       " 'crisis',\n",
       " 'dialogue',\n",
       " 'disclosed',\n",
       " 'dispute',\n",
       " 'disputes',\n",
       " 'due',\n",
       " 'economy',\n",
       " 'effect',\n",
       " 'exploration',\n",
       " 'face-off',\n",
       " 'foreign',\n",
       " 'found',\n",
       " 'greece',\n",
       " 'greek',\n",
       " 'historic',\n",
       " 'including',\n",
       " 'international',\n",
       " 'issue',\n",
       " 'justice',\n",
       " 'last',\n",
       " 'latest',\n",
       " 'legal',\n",
       " 'meet',\n",
       " 'members',\n",
       " 'message',\n",
       " 'minister',\n",
       " 'ministry',\n",
       " 'month',\n",
       " 'nato',\n",
       " 'nazmi',\n",
       " 'negotiations',\n",
       " 'oil',\n",
       " 'opportunity',\n",
       " 'ozal',\n",
       " 'papandreou',\n",
       " 'planned',\n",
       " 'political',\n",
       " 'prime',\n",
       " 'repeatedly',\n",
       " 'reply',\n",
       " 'research',\n",
       " 'rights',\n",
       " 'said',\n",
       " 'sea',\n",
       " 'search',\n",
       " 'security',\n",
       " 'sent',\n",
       " 'shelf',\n",
       " 'solution',\n",
       " 'solve',\n",
       " 'solved',\n",
       " 'statement',\n",
       " 'stemmed',\n",
       " 'territorrial',\n",
       " 'today',\n",
       " 'turgut',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'two',\n",
       " 'waters',\n",
       " 'week',\n",
       " 'work',\n",
       " 'would'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store result of running clean_text\n",
    "clean_text_2 = clean_text(crude_article)\n",
    "\n",
    "# Print unique words (a set of the result)\n",
    "set(clean_text_2)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
